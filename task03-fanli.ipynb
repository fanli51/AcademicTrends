{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 任务说明\n\n- 任务主题：论文代码统计，统计所有论文出现代码的相关统计；\n- 任务内容：使用正则表达式统计代码连接、页数和图表数据；\n- 任务成果：学习正则表达式统计；"},{"metadata":{},"cell_type":"markdown","source":"## 数据处理步骤\n\n在原始arxiv数据集中作者经常会在论文的`comments`或`abstract`字段中给出具体的代码链接，所以我们需要从这些字段里面找出代码的链接。\n\n- 确定数据出现的位置；\n- 使用正则表达式完成匹配；\n- 完成相关的统计；"},{"metadata":{},"cell_type":"markdown","source":"## 正则表达式\n\n正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等。\n\n#### 普通字符：大写和小写字母、所有数字、所有标点符号和一些其他符号\n\n| 字符       | 描述                                                         |\n| ---------- | ------------------------------------------------------------ |\n| **[ABC]**  | 匹配 [...] 中的所有字符，例如 [aeiou] 匹配字符串 \"google runoob taobao\" 中所有的 e o u a 字母。 |\n| **[^ABC]** | 匹配除了 **[...]** 中字符的所有字符，例如 **[^aeiou]** 匹配字符串 \"google runoob taobao\" 中除了 e o u a 字母的所有字母。 |\n| **[A-Z]**  | [A-Z] 表示一个区间，匹配所有大写字母，[a-z] 表示所有小写字母。 |\n| .          | 匹配除换行符（\\n、\\r）之外的任何单个字符，相等于 **[^\\n\\r]**。 |\n| **[\\s\\S]** | 匹配所有。\\s 是匹配所有空白符，包括换行，\\S 非空白符，包括换行。 |\n| **\\w**     | 匹配字母、数字、下划线。等价于 [A-Za-z0-9_]                  |\n\n#### 特殊字符：有特殊含义的字符\n\n| 特别字符 | 描述                                                         |\n| :------- | :----------------------------------------------------------- |\n| ( )      | 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。要匹配这些字符，请使用 \\( 和 \\)。 |\n| *        | 匹配前面的子表达式零次或多次。要匹配 * 字符，请使用 \\*。     |\n| +        | 匹配前面的子表达式一次或多次。要匹配 + 字符，请使用 \\+。     |\n| .        | 匹配除换行符 \\n 之外的任何单字符。要匹配 . ，请使用 \\. 。    |\n| [        | 标记一个中括号表达式的开始。要匹配 [，请使用 \\[。            |\n| ?        | 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符。要匹配 ? 字符，请使用 \\?。 |\n| \\        | 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如， 'n' 匹配字符 'n'。'\\n' 匹配换行符。序列 '\\\\' 匹配 \"\\\"，而 '\\(' 则匹配 \"(\"。 |\n| ^        | 匹配输入字符串的开始位置，除非在方括号表达式中使用，当该符号在方括号表达式中使用时，表示不接受该方括号表达式中的字符集合。要匹配 ^ 字符本身，请使用 \\^。 |\n| {        | 标记限定符表达式的开始。要匹配 {，请使用 \\{。                |\n| \\|       | 指明两项之间的一个选择。要匹配 \\|，请使用 \\|。               |\n\n#### 限定符\n\n| 字符  | 描述                                                         |\n| :---- | :----------------------------------------------------------- |\n| *     | 匹配前面的子表达式零次或多次。例如，zo* 能匹配 \"z\" 以及 \"zoo\"。* 等价于{0,}。 |\n| +     | 匹配前面的子表达式一次或多次。例如，'zo+' 能匹配 \"zo\" 以及 \"zoo\"，但不能匹配 \"z\"。+ 等价于 {1,}。 |\n| ?     | 匹配前面的子表达式零次或一次。例如，\"do(es)?\" 可以匹配 \"do\" 、 \"does\" 中的 \"does\" 、 \"doxy\" 中的 \"do\" 。? 等价于 {0,1}。 |\n| {n}   | n 是一个非负整数。匹配确定的 n 次。例如，'o{2}' 不能匹配 \"Bob\" 中的 'o'，但是能匹配 \"food\" 中的两个 o。 |\n| {n,}  | n 是一个非负整数。至少匹配n 次。例如，'o{2,}' 不能匹配 \"Bob\" 中的 'o'，但能匹配 \"foooood\" 中的所有 o。'o{1,}' 等价于 'o+'。'o{0,}' 则等价于 'o*'。 |\n| {n,m} | m 和 n 均为非负整数，其中n <= m。最少匹配 n 次且最多匹配 m 次。例如，\"o{1,3}\" 将匹配 \"fooooood\" 中的前三个 o。'o{0,1}' 等价于 'o?'。请注意在逗号和两个数之间不能有空格。 |\n\n## 具体代码实现以及讲解\n\n首先我们来统计论文页数，也就是在`comments`字段中抽取pages和figures和个数，首先完成字段读取。\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#正则表达式常用例子\n'''\n首先，从最基础的正则表达式说起。\n假设我们的想法是把一个字符串中的所有\"python\"给匹配到。我们试一试怎么做\n'''\nimport re\n\nkey = r\"javapythonhtmlvhdl\"#这是源文本\np1 = r\"python\"#这是我们写的正则表达式\npattern1 = re.compile(p1)#同样是编译\nmatcher1 = re.search(pattern1,key)#同样是查询\nprint(matcher1.group(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############初级####\nimport re\n\nkey = r\"<h1>hello world<h1>\"#源文本\np1 = r\"<h1>.+<h1>\"#我们写的正则表达式，下面会将为什么\npattern1 = re.compile(p1)\nprint(pattern1.findall(key))#发没发现，我怎么写成findall了？咋变了呢？\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":".字符在正则表达式代表着可以代表任何一个字符（包括它本身）\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkey = r\"afiouwehrfuichuxiuhong@xxx.edu.cnaskdjhfiosueh\"\np1 = r\"chuxiuhong@xxx\\.edu\\.cn\"\npattern1 = re.compile(p1)\nprint(pattern1.findall(key))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":".字符在正则表达式代表着可以代表任何一个字符（包括它本身）”，但是\"hello world\"可不是一个字符啊。\n+的作用是将前面一个字符或一个子表达式重复一遍或者多遍。\n比方说表达式“ab+”那么它能匹配到“abbbbb”，但是不能匹配到\"a\"，它要求你必须得有个b，多了不限，少了不行。你如果问我有没有那种“有没有都行，有多少都行的表达方式”，回答是有的。\n*跟在其他符号后面表达可以匹配到它0次或多次\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkey = r\"http://www.nsfbuhwe.com and https://www.auhfisna.com\"#胡编乱造的网址，别在意\np1 = r\"https*://\"#看那个星号！\npattern1 = re.compile(p1)\nprint(pattern1.findall(key))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.比方说我们有这么一个字符串\"cat hat mat qat\"，如果你本来就知道\"at\"前面是c、h、m其中之一时这才构成单词，你想把这样的匹配出来。\n[]代表匹配里面的字符中的任意一个\n还是举个栗子，我们发现啊，有的程序员比较过分，，在<html></html>这对标签上，大小写混用，"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkey = r\"lalala<hTml>hello</Html>heiheihei\"\np1 = r\"<[Hh][Tt][Mm][Ll]>.+?</[Hh][Tt][Mm][Ll]>\"\npattern1 = re.compile(p1)\nprint(pattern1.findall(key) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我们既然有了范围性的匹配，自然有范围性的排除。\n\n[^]代表除了内部包含的字符以外都能匹配\n还是cat,hat,mat,qat这个例子，我们想匹配除了qat以外的，那么就应该这么写："},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkey = r\"mat cat hat pat\"\np1 = r\"[^p]at\"#这代表除了p以外都匹配\npattern1 = re.compile(p1)\nprint( pattern1.findall(key))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3..+?  与 .+区别"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkey = r\"chuxiuhong@hit.edu.cn\"\np1 = r\"@.+\\.\"#我想匹配到@后面一直到“.”之间的，在这里是hit\npattern1 = re.compile(p1)\nprint(pattern1.findall(key))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkey = r\"chuxiuhong@hit.edu.cn\"\np1 = r\"@.+?\\.\"#我想匹配到@后面一直到“.”之间的，在这里是hit\npattern1 = re.compile(p1)\nprint(pattern1.findall(key))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"加了一个“?”我们就将贪婪的“+”改成了懒惰的“+”。这对于[abc]+,\\w*之类的同样适用。\n\n小测验：上面那个例子可以不使用懒惰匹配，想一种方法得到同样的结果\n\n个人建议：在你使用\"+\",\"*\"的时候，一定先想好到底是用贪婪型还是懒惰型，尤其是当你用到范围较大的项目上时，因为很有可能它就多匹配字符回来给你！！！\n\n为了能够准确的控制重复次数，正则表达式还提供\n{a,b}(代表a<=匹配次数<=b)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkey = r\"saas and sas and saaas\"\np1 = r\"sa{1,2}s\"\npattern1 = re.compile(p1)\nprint(pattern1.findall(key))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nkey = \"A. S. Mishchenko (1 and 2) and N. Nagaosa (1 and 3) ((1) CREST, Japan\\n  Science and Technology Agency, (2) Russian Research Centre ``Kurchatov\\n  Institute'', (3) The University of Tokyo)\"\np1 = r\"[(].*?[)]\"#我想匹配到@后面一直到“.”之间的，在这里是hit\npattern1 = re.compile(p1)\nprint(pattern1.findall(key))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#例子：将字符串切分处理\nimport re\n#把括号内的内容替换为空\nt=\"A. S. Mishchenko (1 and 2) and N. Nagaosa (1 and 3) ((1) CREST, Japan\\n  Science and Technology Agency, (2) Russian Research Centre ``Kurchatov\\n  Institute'', (3) The University of Tokyo)\"\n#t='Peter Brommer and Franz G\\\\\"ahler (Institut f\\\\\"ur Theoretische und\\n  Angewandte Physik, Universit\\\\\"at Stuttgart)'\n#删除\\n\nt1=re.sub(r\"[\\n]\", \"\",t)\n#删除((。。。。)内容\nt1=re.sub(r\"[(]{2}(.*?)[)]$\", \"\",t1)\n#删除()内容\nt1=re.sub(r\"[(](.*?)[)]\", \"\",t1)\nr1=[]\n#切分 ， and 两种情况切分\nt2=re.split(\",| and \",t1)\nfor j in range(len(t2)):\n    temp=re.split(\" \",str.strip(t2[j]))\n    temp = temp[-1:] + temp[:-1]\n#    print(temp)  \n    r1.append(temp)\n\nr1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"解释一下：\n1.正则匹配串前加了r就是为了使得里面的特殊符号不用写反斜杠了。\n\n2.[ ]具有去特殊符号的作用,也就是说[(]里的(只是平凡的括号\n\n3.正则匹配串里的()是为了提取整个正则串中符合括号里的正则的内容\n\n.是为了表示除了换行符的任一字符。*克林闭包，出现0次或无限次。\n\n加了？是最小匹配，不加是贪婪匹配。\n\nre.S是为了让.表示除了换行符的任一字符。\n\nPS：这里再为大家提供2款非常方便的正则表达式工具供大家参考使用："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:29:48.711453Z","start_time":"2021-01-02T07:29:48.059043Z"},"trusted":true},"cell_type":"code","source":"# 导入所需的package\nimport seaborn as sns #用于画图\nfrom bs4 import BeautifulSoup #用于爬取arxiv的数据\nimport re #用于正则表达式，匹配字符串的模式\nimport requests #用于网络连接，发送网络请求，使用域名获取对应信息\nimport json #读取数据，我们的数据为json格式的\nimport pandas as pd #数据处理，数据分析\nimport matplotlib.pyplot as plt #画图工具","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:30:10.507358Z","start_time":"2021-01-02T07:29:49.67605Z"},"trusted":true},"cell_type":"code","source":"def readArxivFile(path, columns=['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n       'report-no', 'categories', 'license', 'abstract', 'versions',\n       'update_date', 'authors_parsed'], count=None):\n    '''\n    定义读取文件的函数\n        path: 文件路径\n        columns: 需要选择的列\n        count: 读取行数\n    '''\n    \n    data  = []\n    with open(path, 'r') as f: \n        for idx, line in enumerate(f): \n            if idx == count:\n                break\n                \n            d = json.loads(line)\n            d = {col : d[col] for col in columns}\n            data.append(d)\n\n    data = pd.DataFrame(data)\n    return data\n\n#data = readArxivFile('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json', ['id', 'abstract', 'categories', 'comments'])\n#data = readArxivFile('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json')\ndata = readArxivFile('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json',['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n       'report-no', 'categories', 'license', 'abstract', 'versions',\n       'update_date', 'authors_parsed'])\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"对pages进行抽取："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:30:15.19911Z","start_time":"2021-01-02T07:30:10.718931Z"},"trusted":true},"cell_type":"code","source":"# 使用正则表达式匹配，XX pages\ndata['pages'] = data['comments'].apply(lambda x: re.findall('[1-9][0-9]* pages', str(x)))\n\n# 筛选出有pages的论文\ndata = data[data['pages'].apply(len) > 0]\n\n# 由于匹配得到的是一个list，如['19 pages']，需要进行转换\ndata['pages'] = data['pages'].apply(lambda x: float(x[0].replace(' pages', '')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"对pages进行统计，统计结果如下：论文平均的页数为17页，75%的论文在22页以内，最长的论文有11232页。"},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:30:27.468809Z","start_time":"2021-01-02T07:30:27.383009Z"},"trusted":true},"cell_type":"code","source":"data['pages'].describe().astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"接下来按照分类统计论文页数，选取了论文的第一个类别的主要类别："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:30:59.170351Z","start_time":"2021-01-02T07:30:58.096126Z"},"trusted":true},"cell_type":"code","source":"# 选择主要类别\ndata['categories'] = data['categories'].apply(lambda x: x.split(' ')[0])\ndata['categories'] = data['categories'].apply(lambda x: x.split('.')[0])\n\n# 每类论文的平均页数\nplt.figure(figsize=(12, 6))\ndata.groupby(['categories'])['pages'].mean().plot(kind='bar')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"接下来对论文图表个数进行抽取："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:31:16.225134Z","start_time":"2021-01-02T07:31:12.823092Z"},"trusted":true},"cell_type":"code","source":"data['figures'] = data['comments'].apply(lambda x: re.findall('[1-9][0-9]* figures', str(x)))\ndata = data[data['figures'].apply(len) > 0]\ndata['figures'] = data['figures'].apply(lambda x: float(x[0].replace(' figures', '')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最后我们对论文的代码链接进行提取，为了简化任务我们只抽取github链接：\n"},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:32:16.121702Z","start_time":"2021-01-02T07:32:15.033667Z"},"trusted":true},"cell_type":"code","source":"# 筛选包含github的论文\ndata_with_code = data[\n    (data.comments.str.contains('github')==True)|\n                      (data.abstract.str.contains('github')==True)\n]\ndata_with_code['text'] = data_with_code['abstract'].fillna('') + data_with_code['comments'].fillna('')\n\n# 使用正则表达式匹配论文\npattern = '[a-zA-z]+://github[^\\s]*'\ndata_with_code['code_flag'] = data_with_code['text'].str.findall(pattern).apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"并对论文按照类别进行绘图："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:32:29.528795Z","start_time":"2021-01-02T07:32:29.374662Z"},"trusted":true},"cell_type":"code","source":"data_with_code = data_with_code[data_with_code['code_flag'] == 1]\nplt.figure(figsize=(12, 6))\ndata_with_code.groupby(['categories'])['code_flag'].count().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#选择近2年的数据\n#我们的任务要求对于2019年以后的paper进行分析，所以首先对于时间特征进行预处理，从而得到2019年以后的所有种类的论文：\n\ndata[\"year\"] = pd.to_datetime(data[\"update_date\"]).dt.year #将update_date从例如2019-02-20的str变为datetime格式，并提取处year\ndel data[\"update_date\"] #删除 update_date特征，其使命已完成\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 选择时间从2019年以后的下面的论文\ndata2 = data[(data['year'].apply(lambda x: x>=2019)) & (data['journal-ref'].isnull()==False)]\ndata2.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['categories'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['categories'].value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}