{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 任务说明\n\n- 学习主题：论文分类（数据建模任务），利用已有数据建模，对新论文进行类别分类；\n- 学习内容：使用论文标题完成类别分类；\n- 学习成果：学会文本分类的基本方法、`TF-IDF`等；"},{"metadata":{},"cell_type":"markdown","source":"## 数据处理步骤\n\n在原始arxiv论文中论文都有对应的类别，而论文类别是作者填写的。在本次任务中我们可以借助论文的标题和摘要完成：\n\n- 对论文标题和摘要进行处理；\n- 对论文类别进行处理；\n- 构建文本分类模型；"},{"metadata":{},"cell_type":"markdown","source":"## 文本分类思路\n\n- 思路1：TF-IDF+机器学习分类器\n\n直接使用TF-IDF对文本提取特征，使用分类器进行分类，分类器的选择上可以使用SVM、LR、XGboost等\n\n- 思路2：FastText\n\nFastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建分类器\n\n- 思路3：WordVec+深度学习分类器\n\nWordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRnn或者BiLSTM。\n\n- 思路4：Bert词向量\n\nBert是高配款的词向量，具有强大的建模学习能力。"},{"metadata":{},"cell_type":"markdown","source":"## 具体代码实现以及讲解\n\n为了方便大家入门文本分类，我们选择思路1和思路2给大家讲解。首先完成字段读取："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:37:06.067689Z","start_time":"2021-01-02T07:37:05.413594Z"},"trusted":true},"cell_type":"code","source":"# 导入所需的package\nimport seaborn as sns #用于画图\nfrom bs4 import BeautifulSoup #用于爬取arxiv的数据\nimport re #用于正则表达式，匹配字符串的模式\nimport requests #用于网络连接，发送网络请求，使用域名获取对应信息\nimport json #读取数据，我们的数据为json格式的\nimport pandas as pd #数据处理，数据分析\nimport matplotlib.pyplot as plt #画图工具","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:38:47.791291Z","start_time":"2021-01-02T07:38:45.515867Z"},"trusted":true},"cell_type":"code","source":"def readArxivFile(path, columns=['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n       'report-no', 'categories', 'license', 'abstract', 'versions',\n       'update_date', 'authors_parsed'], count=None):\n    '''\n    定义读取文件的函数\n        path: 文件路径\n        columns: 需要选择的列\n        count: 读取行数\n    '''\n    \n    data  = []\n    with open(path, 'r') as f: \n        for idx, line in enumerate(f): \n            if idx == count:\n                break\n                \n            d = json.loads(line)\n            d = {col : d[col] for col in columns}\n            data.append(d)\n\n    data = pd.DataFrame(data)\n    return data\n\ndata = readArxivFile('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json', \n                     ['id', 'title', 'categories', 'abstract'],\n                    200000)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"为了方便数据的处理，我们可以将标题和摘要拼接一起完成分类。"},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:39:04.746931Z","start_time":"2021-01-02T07:39:04.199655Z"},"trusted":true},"cell_type":"code","source":"data['text'] = data['title'] + data['abstract']\n\ndata['text'] = data['text'].apply(lambda x: x.replace('\\n',' '))\ndata['text'] = data['text'].apply(lambda x: x.lower())\ndata = data.drop(['abstract', 'title'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"由于原始论文有可能有多个类别，所以也需要处理："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:39:15.639828Z","start_time":"2021-01-02T07:39:15.214064Z"},"trusted":true},"cell_type":"code","source":"# 多个类别，包含子分类\ndata['categories'] = data['categories'].apply(lambda x : x.split(' '))\n\n# 单个类别，不包含子分类\ndata['categories_big'] = data['categories'].apply(lambda x : [xx.split('.')[0] for xx in x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"然后将类别进行编码，这里类别是多个，所以需要多编码："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:39:32.136609Z","start_time":"2021-01-02T07:39:31.088518Z"},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ndata_label = mlb.fit_transform(data['categories_big'].iloc[:])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 思路1\n\n思路1使用TFIDF提取特征，限制最多4000个单词："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:40:19.903548Z","start_time":"2021-01-02T07:40:07.053896Z"},"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features=4000)\ndata_tfidf = vectorizer.fit_transform(data['text'].iloc[:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"由于这里是多标签分类，可以使用sklearn的多标签分类进行封装："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:41:42.35903Z","start_time":"2021-01-02T07:41:40.804323Z"},"trusted":true},"cell_type":"code","source":"# 划分训练集和验证集\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data_tfidf, data_label,\n                                                 test_size = 0.2,random_state = 1)\n\n# 构建多标签分类模型\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultiOutputClassifier(MultinomialNB()).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:41:48.342696Z","start_time":"2021-01-02T07:41:48.063639Z"},"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, clf.predict(x_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 思路2\n\n思路2使用深度学习模型，单词进行词嵌入然后训练。将数据集处理进行编码，并进行截断："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T07:57:52.147577Z","start_time":"2021-01-02T07:57:52.122238Z"},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data['text'].iloc[:100000], \n                                                    data_label[:100000],\n                                                 test_size = 0.95,random_state = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T08:00:14.205263Z","start_time":"2021-01-02T08:00:03.24602Z"},"trusted":true},"cell_type":"code","source":"# parameter\nmax_features= 500\nmax_len= 150\nembed_size=100\nbatch_size = 128\nepochs = 5\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n\ntokens = Tokenizer(num_words = max_features)\ntokens.fit_on_texts(list(data['text'].iloc[:100000]))\n\ny_train = data_label[:100000]\nx_sub_train = tokens.texts_to_sequences(data['text'].iloc[:100000])\nx_sub_train = sequence.pad_sequences(x_sub_train, maxlen=max_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameter\nmax_features= 500\nmax_len= 150\nembed_size=100\nbatch_size = 128\nepochs = 5\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n\ntokens = Tokenizer(num_words = max_features)\ntokens.fit_on_texts(list(data['text'].iloc[:100000]))\n\nx_sub_train = tokens.texts_to_sequences(x_train)\nx_sub_train = sequence.pad_sequences(x_sub_train, maxlen=max_len)\n\nx_sub_test = tokens.texts_to_sequences(x_test)\nx_sub_test = sequence.pad_sequences(x_sub_test, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"定义模型并完成训练："},{"metadata":{"ExecuteTime":{"end_time":"2021-01-02T08:08:55.690388Z","start_time":"2021-01-02T08:00:19.943791Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"# LSTM model\n# Keras Layers:\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D# Keras Callback Functions:\nfrom keras.callbacks import Callback\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\nsequence_input = Input(shape=(max_len, ))\nx = Embedding(max_features, embed_size, trainable=True)(sequence_input)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nx = concatenate([avg_pool, max_pool]) \npreds = Dense(19, activation=\"sigmoid\")(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\nmodel.fit(x_sub_train, y_train, \n          batch_size=batch_size, \n          validation_split=0.2,\n          epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\npredict_model=model.predict(x_sub_test)\npredict_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertForSequenceClassification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.utils.data as Data\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\npath = '/kaggle/input/ml2019fall-hw5'\ntrainx_path = os.path.join(path, 'train_x.csv')\ntrainy_path = os.path.join(path, 'train_y.csv')\ntestx_path = os.path.join(path, 'test_x.csv')\ntrainx = pd.read_csv(trainx_path)\ntrainy = pd.read_csv(trainy_path)\ntestx = pd.read_csv(testx_path)\ntrainx['comment'].iloc[0]\ntrainy.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}